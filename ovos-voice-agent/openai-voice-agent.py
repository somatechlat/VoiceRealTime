#!/usr/bin/env python3
"""
OpenAI-Compatible Voice Agent
Based on OpenAI Realtime API specification for speech-to-speech voice agents
https://platform.openai.com/docs/guides/voice-agents

This implements the complete OpenAI Realtime API with:
- Server VAD (Voice Activity Detection)
- Real-time speech streaming
- Turn-based conversations
- Audio input/output streaming
- Function calling support
"""

import asyncio
import json
import uuid
import base64
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
import websockets
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
import uvicorn

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import our LLM integration
try:
    from llm_integration import generate_ai_response
    logger.info("‚úÖ LLM integration loaded")
except ImportError:
    logger.warning("‚ö†Ô∏è LLM integration not available - using mock responses")
    
    async def generate_ai_response(session_id: str, text: str) -> str:
        return f"I heard you say: {text}. This is a mock response since LLM integration is not available."

app = FastAPI(title="OpenAI-Compatible Voice Agent", version="1.0.0")

class VoiceSession:
    """Manages a voice conversation session"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.conversation = []
        self.audio_buffer = b''
        self.is_speaking = False
        self.turn_detection = {
            "type": "server_vad",
            "threshold": 0.5,
            "prefix_padding_ms": 300,
            "silence_duration_ms": 500
        }
        self.voice = "alloy"
        self.instructions = "You are a helpful AI assistant. Respond naturally and conversationally."

class RealtimeConnectionManager:
    """Manages WebSocket connections for the Realtime API"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.sessions: Dict[str, VoiceSession] = {}
    
    async def connect(self, websocket: WebSocket, session_id: str):
        """Connect a new WebSocket session"""
        await websocket.accept()
        self.active_connections[session_id] = websocket
        self.sessions[session_id] = VoiceSession(session_id)
        
        # Send session.created event
        await self.send_event(session_id, {
            "type": "session.created",
            "session": {
                "id": session_id,
                "object": "realtime.session",
                "model": "gpt-4o-realtime-preview-2024-10-01",
                "expires_at": int(datetime.now().timestamp()) + 900,  # 15 minutes
                "modalities": ["text", "audio"],
                "instructions": self.sessions[session_id].instructions,
                "voice": self.sessions[session_id].voice,
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": self.sessions[session_id].turn_detection,
                "tools": [],
                "tool_choice": "auto",
                "temperature": 0.8,
                "max_response_output_tokens": "inf"
            }
        })
        
        logger.info(f"‚úÖ Voice session {session_id} connected")
    
    def disconnect(self, session_id: str):
        """Disconnect a WebSocket session"""
        if session_id in self.active_connections:
            del self.active_connections[session_id]
        if session_id in self.sessions:
            del self.sessions[session_id]
        logger.info(f"‚ùå Voice session {session_id} disconnected")
    
    async def send_event(self, session_id: str, event: Dict[str, Any]):
        """Send an event to a WebSocket connection"""
        if session_id in self.active_connections:
            try:
                websocket = self.active_connections[session_id]
                await websocket.send_text(json.dumps(event))
            except Exception as e:
                logger.error(f"Failed to send event to {session_id}: {e}")

class VoiceEventProcessor:
    """Processes voice events according to OpenAI Realtime API"""
    
    def __init__(self, connection_manager: RealtimeConnectionManager):
        self.manager = connection_manager
    
    async def process_event(self, session_id: str, event: Dict[str, Any]):
        """Process incoming events from the client"""
        event_type = event.get("type")
        session = self.manager.sessions.get(session_id)
        
        if not session:
            logger.error(f"No session found for {session_id}")
            return
        
        logger.info(f"üì° Processing event: {event_type}")
        
        try:
            if event_type == "session.update":
                await self.handle_session_update(session_id, event)
            
            elif event_type == "input_audio_buffer.append":
                await self.handle_audio_append(session_id, event)
            
            elif event_type == "input_audio_buffer.commit":
                await self.handle_audio_commit(session_id, event)
            
            elif event_type == "input_audio_buffer.clear":
                await self.handle_audio_clear(session_id, event)
            
            elif event_type == "conversation.item.create":
                await self.handle_conversation_item_create(session_id, event)
            
            elif event_type == "response.create":
                await self.handle_response_create(session_id, event)
            
            elif event_type == "response.cancel":
                await self.handle_response_cancel(session_id, event)
            
            else:
                logger.warning(f"‚ùì Unknown event type: {event_type}")
                
        except Exception as e:
            logger.error(f"‚ùå Error processing event {event_type}: {e}")
            await self.send_error(session_id, "processing_error", str(e))
    
    async def handle_session_update(self, session_id: str, event: Dict[str, Any]):
        """Handle session configuration updates"""
        session = self.manager.sessions[session_id]
        session_config = event.get("session", {})
        
        # Update session configuration
        if "instructions" in session_config:
            session.instructions = session_config["instructions"]
        if "voice" in session_config:
            session.voice = session_config["voice"]
        if "turn_detection" in session_config:
            session.turn_detection.update(session_config["turn_detection"])
        
        # Send session.updated
        await self.manager.send_event(session_id, {
            "type": "session.updated",
            "session": {
                "id": session_id,
                "object": "realtime.session",
                "model": "gpt-4o-realtime-preview-2024-10-01",
                "instructions": session.instructions,
                "voice": session.voice,
                "turn_detection": session.turn_detection
            }
        })
    
    async def handle_audio_append(self, session_id: str, event: Dict[str, Any]):
        """Handle incoming audio data"""
        session = self.manager.sessions[session_id]
        audio_b64 = event.get("audio", "")
        
        if audio_b64:
            try:
                audio_data = base64.b64decode(audio_b64)
                session.audio_buffer += audio_data
                
                # Server VAD simulation - detect speech start
                if not session.is_speaking and len(session.audio_buffer) > 1024:
                    session.is_speaking = True
                    await self.manager.send_event(session_id, {
                        "type": "input_audio_buffer.speech_started",
                        "audio_start_ms": int(datetime.now().timestamp() * 1000),
                        "item_id": f"item_{uuid.uuid4().hex[:8]}"
                    })
                
                logger.debug(f"üìä Audio buffer size: {len(session.audio_buffer)} bytes")
                
            except Exception as e:
                logger.error(f"Failed to decode audio: {e}")
    
    async def handle_audio_commit(self, session_id: str, event: Dict[str, Any]):
        """Handle audio buffer commit - triggers transcription and response"""
        session = self.manager.sessions[session_id]
        
        if session.is_speaking:
            session.is_speaking = False
            
            # Send speech stopped
            await self.manager.send_event(session_id, {
                "type": "input_audio_buffer.speech_stopped",
                "audio_end_ms": int(datetime.now().timestamp() * 1000),
                "item_id": f"item_{uuid.uuid4().hex[:8]}"
            })
            
            # Send committed
            await self.manager.send_event(session_id, {
                "type": "input_audio_buffer.committed",
                "previous_item_id": None,
                "item_id": f"item_{uuid.uuid4().hex[:8]}"
            })
            
            # Simulate transcription (in real implementation, use Whisper)
            transcribed_text = await self.transcribe_audio(session.audio_buffer)
            
            # Create conversation item for user input
            user_item_id = f"item_{uuid.uuid4().hex[:8]}"
            user_item = {
                "id": user_item_id,
                "object": "realtime.item",
                "type": "message",
                "status": "completed",
                "role": "user",
                "content": [{
                    "type": "input_audio",
                    "transcript": transcribed_text
                }]
            }
            
            # Send conversation.item.created
            await self.manager.send_event(session_id, {
                "type": "conversation.item.created",
                "previous_item_id": None,
                "item": user_item
            })
            
            session.conversation.append(user_item)
            
            # Clear audio buffer
            session.audio_buffer = b''
            
            # Automatically trigger response
            await self.handle_response_create(session_id, {"type": "response.create"})
    
    async def handle_audio_clear(self, session_id: str, event: Dict[str, Any]):
        """Handle audio buffer clear"""
        session = self.manager.sessions[session_id]
        session.audio_buffer = b''
        session.is_speaking = False
        
        await self.manager.send_event(session_id, {
            "type": "input_audio_buffer.cleared"
        })
    
    async def handle_conversation_item_create(self, session_id: str, event: Dict[str, Any]):
        """Handle conversation item creation"""
        session = self.manager.sessions[session_id]
        item = event.get("item", {})
        
        session.conversation.append(item)
        
        await self.manager.send_event(session_id, {
            "type": "conversation.item.created",
            "previous_item_id": session.conversation[-2]["id"] if len(session.conversation) > 1 else None,
            "item": item
        })
    
    async def handle_response_create(self, session_id: str, event: Dict[str, Any]):
        """Handle response generation"""
        session = self.manager.sessions[session_id]
        response_id = f"resp_{uuid.uuid4().hex[:8]}"
        item_id = f"item_{uuid.uuid4().hex[:8]}"
        
        # Send response.created
        await self.manager.send_event(session_id, {
            "type": "response.created",
            "response": {
                "id": response_id,
                "object": "realtime.response",
                "status": "in_progress",
                "status_details": None,
                "output": [],
                "usage": None
            }
        })
        
        # Generate AI response
        try:
            # Get the last user message
            user_messages = [msg for msg in session.conversation if msg.get("role") == "user"]
            if user_messages:
                last_message = user_messages[-1]
                user_text = last_message["content"][0].get("transcript", "")
            else:
                user_text = "Hello"
            
            ai_response = await generate_ai_response(session_id, user_text)
            
            # Create response item
            response_item = {
                "id": item_id,
                "object": "realtime.item",
                "type": "message",
                "status": "completed",
                "role": "assistant",
                "content": [{
                    "type": "audio",
                    "transcript": ai_response
                }]
            }
            
            # Send output item added
            await self.manager.send_event(session_id, {
                "type": "response.output_item.added",
                "response_id": response_id,
                "output_index": 0,
                "item": response_item
            })
            
            # Send audio transcript delta (streaming effect)
            await self.manager.send_event(session_id, {
                "type": "response.audio_transcript.delta",
                "response_id": response_id,
                "item_id": item_id,
                "output_index": 0,
                "content_index": 0,
                "delta": ai_response
            })
            
            # Generate and send audio response
            audio_data = await self.generate_speech(ai_response, session.voice)
            if audio_data:
                # Send audio delta
                audio_b64 = base64.b64encode(audio_data).decode('utf-8')
                await self.manager.send_event(session_id, {
                    "type": "response.audio.delta",
                    "response_id": response_id,
                    "item_id": item_id,
                    "output_index": 0,
                    "content_index": 0,
                    "delta": audio_b64
                })
                
                # Send audio done
                await self.manager.send_event(session_id, {
                    "type": "response.audio.done",
                    "response_id": response_id,
                    "item_id": item_id,
                    "output_index": 0,
                    "content_index": 0
                })
            
            # Send response done
            await self.manager.send_event(session_id, {
                "type": "response.done",
                "response": {
                    "id": response_id,
                    "object": "realtime.response",
                    "status": "completed",
                    "status_details": {"type": "completed"},
                    "output": [response_item],
                    "usage": {
                        "total_tokens": len(ai_response.split()),
                        "input_tokens": len(user_text.split()) if 'user_text' in locals() else 0,
                        "output_tokens": len(ai_response.split())
                    }
                }
            })
            
            # Add to conversation
            session.conversation.append(response_item)
            
        except Exception as e:
            logger.error(f"Response generation failed: {e}")
            await self.send_error(session_id, "response_error", str(e))
    
    async def handle_response_cancel(self, session_id: str, event: Dict[str, Any]):
        """Handle response cancellation"""
        # Implementation for response cancellation
        pass
    
    async def transcribe_audio(self, audio_data: bytes) -> str:
        """Transcribe audio to text (mock implementation)"""
        # In a real implementation, this would use Whisper or similar
        if len(audio_data) > 1024:
            return "Hello, I'm speaking to the voice agent!"
        return "..."
    
    async def generate_speech(self, text: str, voice: str) -> bytes:
        """Generate speech from text (mock implementation)"""
        # In a real implementation, this would use TTS
        # Return some mock audio data
        return b'\x00' * 2048  # 2KB of silence
    
    async def send_error(self, session_id: str, error_type: str, message: str):
        """Send error event"""
        await self.manager.send_event(session_id, {
            "type": "error",
            "error": {
                "type": error_type,
                "code": "processing_error",
                "message": message,
                "param": None,
                "event_id": None
            }
        })

# Initialize managers
connection_manager = RealtimeConnectionManager()
event_processor = VoiceEventProcessor(connection_manager)

@app.websocket("/v1/realtime")
async def websocket_realtime(websocket: WebSocket):
    """OpenAI Realtime API WebSocket endpoint"""
    session_id = f"sess_{uuid.uuid4().hex[:24]}"
    
    try:
        await connection_manager.connect(websocket, session_id)
        
        while True:
            try:
                data = await websocket.receive_text()
                event = json.loads(data)
                await event_processor.process_event(session_id, event)
                
            except json.JSONDecodeError as e:
                logger.error(f"Invalid JSON received: {e}")
                await event_processor.send_error(session_id, "invalid_json", str(e))
            except Exception as e:
                logger.error(f"Processing error: {e}")
                await event_processor.send_error(session_id, "processing_error", str(e))
                
    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected: {session_id}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        connection_manager.disconnect(session_id)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "OpenAI-Compatible Voice Agent",
        "active_sessions": len(connection_manager.active_connections),
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "api_compatibility": "OpenAI Realtime API v1"
    }

@app.get("/")
async def root():
    """Serve voice chat interface"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>OpenAI-Compatible Voice Agent</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body style="font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px;">
        <h1>üé§ OpenAI-Compatible Voice Agent</h1>
        <p>Click "Connect" then use the microphone button to start voice conversation.</p>
        <p><strong>Instructions:</strong></p>
        <ol>
            <li>Click "Connect to Voice Agent"</li>
            <li>Click the üé§ microphone button</li>
            <li>Speak your message</li>
            <li>Click üé§ again to stop and get AI response</li>
        </ol>
        <div style="margin: 20px 0;">
            <button id="connectBtn" onclick="window.open('simple-chat.html', '_blank')" 
                    style="padding: 10px 20px; font-size: 16px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer;">
                üöÄ Open Voice Chat
            </button>
        </div>
        <div style="margin-top: 30px; padding: 15px; background: #f8f9fa; border-radius: 5px;">
            <h3>üõ†Ô∏è System Status</h3>
            <p><strong>WebSocket:</strong> ws://localhost:65000/v1/realtime</p>
            <p><strong>Health:</strong> <a href="/health">http://localhost:65000/health</a></p>
            <p><strong>Compatible with:</strong> OpenAI Realtime API v1</p>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

if __name__ == "__main__":
    print("üé§ OpenAI-Compatible Voice Agent - Starting...")
    print("üåê WebSocket: ws://localhost:65000/v1/realtime")
    print("üìã Health: http://localhost:65000/health")
    print("üöÄ Interface: http://localhost:65000/")
    print("‚úÖ Compatible with OpenAI Realtime API v1")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=65000,
        log_level="info"
    )